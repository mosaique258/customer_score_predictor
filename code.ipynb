{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Score Predictor\n",
    "\n",
    "**Overview:** This notebook implements a customer review score predictor using a pre-trained BERT model fine-tuned on a Yelp review dataset from Hugging Face. The goal is to predict the star rating (1 to 5) of a Yelp review based on the text content. We will load the dataset, preprocess it, fine-tune a BERT model for sequence classification, evaluate its performance using various metrics, and finally build a Gradio application for interactive predictions.\n",
    "\n",
    "This notebook uses a **subset of the original Yelp Review Full dataset for demonstration purposes and to reduce training time.** For optimal performance in a real-world scenario, it is recommended to train the model on the entire dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Install and import the required libraries\n",
    "\n",
    "In this step, we install all the necessary Python libraries required for this project. We will be using libraries from the Hugging Face ecosystem (`transformers`, `datasets`, `evaluate`, `accelerate`), along with standard data science libraries (`numpy`, `scikit-learn`) and `gradio` for building the user interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas transformers datasets evaluate accelerate evaluate scikit-learn gradio matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and Set Random Seeds for Reproducibility\n",
    "\n",
    "# --- Standard Libraries ---\n",
    "import random\n",
    "import time\n",
    "\n",
    "# --- Data Science Libraries ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# --- Hugging Face Libraries ---\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Hugging Face interactions for uploading model\n",
    "from huggingface_hub import login\n",
    "# Replace 'YOUR_HUGGING_FACE_TOKEN' with actual token\n",
    "login(token=\"YOUR_HUGGING_FACE_TOKEN\")\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import upload_folder\n",
    "\n",
    "\n",
    "\n",
    "# --- PyTorch ---\n",
    "import torch\n",
    "\n",
    "# --- Gradio for UI ---\n",
    "import gradio as gr\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Disable parallelism for gradio application\n",
    "\n",
    "\n",
    "# --- Set Random Seeds for Reproducibility ---\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED) # For CUDA GPUs\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Load the dataset from huggingface and inspect the data\n",
    "\n",
    "### Load the dataset\n",
    "\n",
    "- Here, we load the \"yelp_review_full\" dataset from Hugging Face Datasets. This dataset contains Yelp reviews with full star ratings (1 to 5). We will inspect a sample review to understand the data structure and content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Yelp review dataset\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "# Display a sample review (index 100) from the training set\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data inspection\n",
    "\n",
    "Before proceeding with model training, it's important to explore and understand the dataset. In this section, we will:\n",
    "\n",
    "- **Convert to Pandas DataFrame:** Convert the Hugging Face Dataset to Pandas DataFrames for easier data manipulation and analysis.\n",
    "- **Visualize Label Distribution:** Plot the distribution of the 'labels' (star ratings) to check for class imbalances.\n",
    "- **Check for Missing Values:** Examine the dataset for any missing values in the 'text' or 'label' columns.\n",
    "- **Split Test Set into Validation and Test Sets:** Divide the original test set into two: a validation set used during training and a final test set for unbiased evaluation at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training and test datasets to Pandas DataFrames\n",
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "test_df = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "\n",
    "# Visualize the distribution of labels in the training set\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='label', data=train_df)\n",
    "plt.title('Distribution of Star Ratings in Training Set')\n",
    "plt.xlabel('Star Rating (Label)')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xticks(ticks=range(5), labels=['1 Star', '2 Stars', '3 Stars', '4 Stars', '5 Stars']) # More descriptive x-axis labels\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Check for missing values in both training and test sets\n",
    "print(\"Missing values in training set:\")\n",
    "print(train_df.isnull().sum())\n",
    "print(\"\\nMissing values in test set:\")\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the original test set into validation and test sets\n",
    "validation_test_split = dataset[\"test\"].train_test_split(test_size=0.5, seed=42) # 50/50 split\n",
    "\n",
    "validation_dataset = validation_test_split[\"train\"] # Use 'train' split as validation\n",
    "test_dataset = validation_test_split[\"test\"]      # Use 'test' split as final test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Tokenize the data\n",
    "\n",
    "In this step, we will tokenize the text data from the Yelp review dataset. To expedite the training process for this demonstration and due to resource limitations, we are using smaller subsets of the full Yelp Review Full dataset. The [original dataset](https://huggingface.co/datasets/Yelp/yelp_review_full) contains a large number of reviews (training set: 650,000, test set: 50,000). For this notebook, we will use a reduced dataset size of 3000 training samples, 1000 validation samples, and 1000 test samples. For a full-scale project, training on the entire dataset is recommended for optimal model performance.\n",
    "\n",
    "In this step, we will tokenize the text data from the dataset. Tokenization is the process of converting text into numerical tokens that can be understood by the BERT model. We are using the tokenizer associated with the \"google-bert/bert-base-cased\" model.\n",
    "\n",
    "We will use the `map` function from the `datasets` library to apply the tokenization function to the entire dataset in batches for efficiency. We will also set `padding=\"max_length\"` and `truncation=True` to ensure all input sequences are of the same length (or shorter than `max_length`) and to handle reviews that are longer than the model's maximum input length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Create smaller datasets for training, validation, and test\n",
    "# Tokenize the datasets before creating smaller subsets\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(3000))\n",
    "small_validation_dataset = tokenized_validation_dataset.shuffle(seed=42).select(range(1000))\n",
    "small_test_dataset = tokenized_test_dataset.shuffle(seed=42).select(range(1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Select the model and initialize it with 5 classes\n",
    "\n",
    "Here we select the pre-trained \"google-bert/bert-base-cased\" model and initialize it for sequence classification. We specify `num_labels=5` because we are predicting one of 5 star ratings (classes). `AutoModelForSequenceClassification.from_pretrained` loads the pre-trained BERT model and adapts it for classification, adding a classification layer on top. `torch_dtype=\"auto\"` allows automatic handling of data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with 5 classes\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5, torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Create evaluation metrics\n",
    "\n",
    "In this step, we define the evaluation metrics we will use to assess the performance of our fine-tuned model. We load the \"accuracy\", \"recall\", \"precision\", and \"f1\" metrics from the `evaluate` library.\n",
    "We choose these metrics because:\n",
    "\n",
    "- **Accuracy**: Provides a general measure of correctness, indicating the percentage of reviews for which the star rating was predicted correctly _across all classes_. It's a good overall metric, but can be misleading if classes are imbalanced.\n",
    "- **Recall, Precision, and F1-score**: These are especially important in multi-class classification problems like ours, and are useful even if there are class imbalances. We use the macro-average for these metrics to give equal weight to each star rating class, regardless of its frequency in the dataset. This ensures that the performance on less frequent classes is also taken into account.\n",
    "- **Recall (Macro-average)**: Measures the ability of the classifier to correctly identify reviews of _each specific star rating_. For each star rating (1 to 5), recall asks: \"Of all the reviews that _actually_ have this star rating, how many did the model correctly predict?\". High recall for a star rating means the model is good at finding most of the reviews that truly belong to that rating.\n",
    "- **Precision (Macro-average)**: Measures the accuracy of the positive predictions _for each star rating_. For each star rating (1 to 5), precision asks: \"Of all the reviews that the model _predicted_ as having this star rating, how many _actually_ have this star rating?\". High precision for a star rating means that when the model predicts a certain star rating, it's usually correct.\n",
    "- **F1-score (Macro-average)**: Provides a single, balanced measure that combines both precision and recall for _each star rating_. It is the harmonic mean of precision and recall. F1-score is particularly useful when we want to balance precision and recall, and it's often a better single metric to consider than accuracy, especially in cases with imbalanced classes or when both false positives and false negatives are important to consider.\n",
    "\n",
    "Finally, the `compute_metrics` function takes the model's prediction outputs (`eval_pred`) and calculates these metrics. It returns a dictionary containing the calculated metrics which will be used by the Trainer during evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation metrics\n",
    "metric_accuracy = evaluate.load(\"accuracy\")\n",
    "metric_recall = evaluate.load(\"recall\")\n",
    "metric_precision = evaluate.load(\"precision\")\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = metric_accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Calculate macro-averaged recall\n",
    "    recall = metric_recall.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    # Calculate macro-averaged precision\n",
    "    precision = metric_precision.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    # Calculate macro-averaged f1-score\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Finetune the model\n",
    "\n",
    "In this section, we configure the training process using `TrainingArguments`. This class provides many options to customize the training. In this notebook, we have configured the following `TrainingArguments`:\n",
    "\n",
    "- `output_dir`: \"test_trainer\" - Specifies the directory where the model checkpoints and training logs will be saved.\n",
    "- `eval_strategy`: \"steps\" - Sets the evaluation strategy to \"steps\", meaning evaluation will be performed at certain intervals defined by `eval_steps`.\n",
    "- `eval_steps`: 100 - Configures evaluation to be performed every 100 training steps.\n",
    "- `num_train_epochs`: 2 - Sets the number of training epochs to 2. For a more ideally tuned model, 3 to 5 epochs are generally recommended, but 2 is used here for demonstration speed.\n",
    "- `logging_steps`: 100 - Configures training logs to be recorded every 100 steps.\n",
    "\n",
    "We have left many other `TrainingArguments` at their default values. Some of the important default parameters include:\n",
    "\n",
    "- `learning_rate`: 5e-5 - The initial learning rate for the AdamW optimizer.\n",
    "- `per_device_train_batch_size`: 8 - Batch size per device during training.\n",
    "- `per_device_eval_batch_size`: 8 - Batch size for evaluation.\n",
    "- `weight_decay`: 0.0 - No weight decay is applied by default.\n",
    "- `warmup_steps`: 0 - No warmup steps for the learning rate scheduler by default.\n",
    "  For a full list of configurable training arguments, refer to the Hugging Face `TrainingArguments` documentation.\n",
    "\n",
    "We then initialize the `Trainer` class from Hugging Face Transformers. The `Trainer` simplifies the training loop and handles many details for us. We provide it with:\n",
    "\n",
    "- The pre-trained **model** we initialized.\n",
    "- The **training arguments** we just configured.\n",
    "- The **training dataset** (`small_train_dataset`).\n",
    "- The **validation dataset** (`small_validation_dataset`) for evaluation during training.\n",
    "- The `compute_metrics` function to calculate evaluation metrics.\n",
    "\n",
    "Finally, we call `trainer.train()` to start the fine-tuning process. The training progress, evaluation metrics at each evaluation step, and checkpoints will be saved in the `output_dir` specified in `TrainingArguments`. After training, we save the fine-tuned model and tokenizer to local directories for later use in inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",             # Directory to save model checkpoints and logs\n",
    "    eval_strategy=\"steps\",          # Evaluate at specified steps\n",
    "    eval_steps=100,                       # Evaluate every 100 steps\n",
    "    num_train_epochs=2,                  # Set to 2 (ideal 3-5 epochs)\n",
    "    logging_steps=100,                    # Log training information every 100 steps\n",
    "\n",
    ")\n",
    "\n",
    "# Initialize trainer with model, datasets, and metrics\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_validation_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "start_time = time.time() # Start timing\n",
    "trainer.train()\n",
    "training_time = time.time() - start_time # End timing and calculate duration\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "model_save_path = \"./score_prediction_model\"\n",
    "tokenizer_save_path = \"./score_prediction_tokenizer\"\n",
    "\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Evaluate the model\n",
    "\n",
    "After fine-tuning, we perform a detailed evaluation of the model's performance on the evaluation dataset. This goes beyond the metrics reported during training and provides a more comprehensive analysis. We will create the following:\n",
    "\n",
    "1.  **Predictions:** Use the trained model to predict star ratings for the evaluation dataset.\n",
    "2.  **Classification Report:** Print a classification report that includes precision, recall, F1-score, and support for each star rating, as well as macro and weighted averages.\n",
    "3.  **Confusion Matrix:** Display a confusion matrix to visualize the counts of true vs. predicted star ratings.\n",
    "4.  **Evaluation Time:** Measure and report the time taken for the evaluation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_eval_time = time.time() # Start timing evaluation\n",
    "\n",
    "# Get predictions on the test dataset\n",
    "predictions = trainer.predict(small_test_dataset)\n",
    "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "y_true = small_test_dataset[\"label\"]\n",
    "\n",
    "eval_time = time.time() - start_eval_time # End timing and calculate evaluation duration\n",
    "\n",
    "# Generate Classification Report\n",
    "print(\"\\nClassification Report (on Test Set):\")\n",
    "print(classification_report(y_true, y_pred, target_names=[f'Star {i+1}' for i in range(5)]))\n",
    "\n",
    "# Generate Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix (on Test Set):\")\n",
    "print(cm)\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[f'Star {i+1}' for i in range(5)],\n",
    "            yticklabels=[f'Star {i+1}' for i in range(5)])\n",
    "plt.xlabel('Predicted Star Rating')\n",
    "plt.ylabel('True Star Rating')\n",
    "plt.title('Confusion Matrix on Test Set - Yelp Review Star Prediction') # Clarify it's on test set\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEvaluation on Test Set completed in {eval_time:.2f} seconds\") # Print evaluation time and clarify it's test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Set up inference\n",
    "\n",
    "This step prepares the model for inference (making predictions on new, unseen text). We reload the fine-tuned model and tokenizer from the saved directories. We also move the model to the appropriate device ('cuda' if a GPU is available, otherwise 'cpu'). Setting the model to `.eval()` mode is a best practice for inference as it disables training-specific layers like dropout and batch normalization, ensuring consistent predictions.\n",
    "\n",
    "We define a `predict` function that takes text as input, tokenizes it, feeds it to the model, and returns the predicted star rating (as an integer from 0 to 4, corresponding to 1 to 5 stars). We also include a test prediction using sample text to verify the inference setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup inference\n",
    "\n",
    "# Define the paths where model and tokenizer were saved\n",
    "model_reload_path = \"./score_prediction_model\"\n",
    "tokenizer_reload_path = \"./score_prediction_tokenizer\"\n",
    "\n",
    "# Reload the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_reload_path)\n",
    "\n",
    "# Reload the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_reload_path)\n",
    "\n",
    "# Move model to device (CPU or CUDA if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval() # Set model to evaluation mode for inference\n",
    "\n",
    "# Define prediction function for single text input\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    return predictions.item()\n",
    "\n",
    "# Test prediction with sample text\n",
    "sample_text = \"I hated it\"\n",
    "prediction = predict(sample_text)\n",
    "print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Build a gradio application for user input\n",
    "\n",
    "In this step, we build a user-friendly Gradio interface to interact with our score prediction model. Gradio allows us to create a web-based GUI quickly.\n",
    "\n",
    "We load the model and tokenizer again (ensuring they are loaded only once globally for efficiency in the Gradio app). We define a mapping `score_texts` to convert the numerical predicted score (0-4) into a more user-readable text output (e.g., \"1 star out of 5\").\n",
    "\n",
    "The `predict_score_gradio` function is designed for the Gradio interface. It takes text input from the user, uses the `predict` function to get the numerical score, and then retrieves the corresponding text output from `score_texts`.\n",
    "\n",
    "Finally, we create and launch the Gradio interface using `gr.Interface`, specifying the prediction function, input and output types, title, and description. The `iface.launch()` command starts the Gradio server, making the application accessible in a web browser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer globally (once)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./score_prediction_tokenizer\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./score_prediction_model\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Keep device specification for best practice\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Score Text mappings ---\n",
    "score_texts = {\n",
    "    0: \"1 star out of 5\",\n",
    "    1: \"2 stars out of 5\",\n",
    "    2: \"3 stars out of 5\",\n",
    "    3: \"4 stars out of 5\",\n",
    "    4: \"5 stars out of 5\"\n",
    "}\n",
    "\n",
    "# --- Prediction function for Gradio Interface ---\n",
    "def predict_score_gradio(text_input):\n",
    "    inputs = tokenizer(text_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    predicted_score = predictions.item()\n",
    "    output_text = score_texts.get(predicted_score, \"Default Text\")\n",
    "\n",
    "    return output_text\n",
    "\n",
    "# --- Gradio Interface ---\n",
    "iface = gr.Interface(\n",
    "    fn=predict_score_gradio, \n",
    "    inputs=gr.Textbox(label=\"Enter text and let the score predictor come up with a star rating for you:\"),\n",
    "    outputs=gr.Textbox(label=\"Predicted Star Rating\", lines=3),\n",
    "    title=\"Customer satisfaction score predictor\",\n",
    "    description=\"Think of the last time, you went somewhere new. How was it?\"\n",
    "    \n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10 - Import model to Hugging Face\n",
    "\n",
    "In this final step, we upload the model to Hugging face. From there, we can launch the gradio app to make it available publicly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Hugging Face Repository\n",
    "\n",
    "We Hugging Face `HfApi` to manage our repository:\n",
    "- We checks if a repository with the specified ID exists and delete it if it does, ensuring a clean start.\n",
    "- We then creates a new Hugging Face repository with the given ID, ready for use.  This handles the case where the repository didn't exist previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hugging face repository\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "repo_owner = \"mosaique258\"\n",
    "repo_name = \"customer-score-predictor\"\n",
    "repo_id = f\"{repo_owner}/{repo_name}\"\n",
    "\n",
    "# Check if the repository exists\n",
    "try:\n",
    "    # Get the repository information\n",
    "    repo_info = api.repo_info(repo_id)\n",
    "    if repo_info:\n",
    "        # If the repository exists, delete it\n",
    "        api.delete_repo(repo_id)\n",
    "        print(f\"Repository '{repo_id}' exists and has been deleted.\")\n",
    "except Exception as e:\n",
    "    # If the repository does not exist, an exception will be raised\n",
    "    print(f\"Repository '{repo_id}' does not exist. Proceeding to create a new one.\")\n",
    "\n",
    "# Create a new repository\n",
    "api.create_repo(repo_id=repo_id)  # Set private=True if you want a private repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload trained model to Hugging Face Repository\n",
    "\n",
    "We finally upload hte model and the tokenizer information to Hugging Face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload model to repo\n",
    "\n",
    "\n",
    "# Upload the model directory\n",
    "upload_folder(\n",
    "    folder_path=\"score_prediction_model\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "# Upload the tokenizer directory\n",
    "upload_folder(\n",
    "    folder_path=\"score_prediction_tokenizer\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sappho_classification_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
